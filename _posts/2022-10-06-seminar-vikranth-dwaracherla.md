---
layout: post
title:  "[Seminar] 'Epistemic Neural Networks' by Vikranth Dwaracherla"
categories: Seminar
tags: [seminar]
excerpt: "<ul>
<li><b>Title:</b> Epistemic Neural Networks</li>
<li><b>Speaker:</b> Vikranth Dwaracherla (DeepMind Mountain View)</li> 
<li><b>Time and date:</b> 4pm to 5pm, November 2nd, 2022 (Wednesday)</li>
<li><b>Room:</b> <a href='https://qmul-ac-uk.zoom.us/j/85984736226?pwd=MWZKWjRaZjZDWE1CT2dncjVhZXVVZz09'>Virtual (Zoom)</a></li>
</ul>"
mathjax: false
---

* content
{:toc}

<ul>
<li><b>Title:</b> Epistemic Neural Networks</li>
<li><b>Speaker:</b> Vikranth Dwaracherla (DeepMind Mountain View)</li> 
<li><b>Time and date:</b> 4pm to 5pm, November 2nd, 2022 (Wednesday)</li>
<li><b>Room:</b> <a href='https://qmul-ac-uk.zoom.us/j/85984736226?pwd=MWZKWjRaZjZDWE1CT2dncjVhZXVVZz09'>Virtual (Zoom)</a></li>
</ul>

The Game AI Research Group is glad to announce a (virtual) talk by Vikranth Dwaracherla on November 2nd, 2022, at 16:00.

## Abstract

Intelligence relies on an agent's knowledge of what it does not know. This capability relies on the quality of joint predictions of labels across multiple inputs. Conventional neural networks lack this capability and, since most research has focused on marginal predictions, this shortcoming has been largely overlooked.  By assessing the quality of joint predictions it is possible to determine whether the neural network effectively distinguishes between epistemic uncertainty (that is due to lack of knowledge) and aleatoric uncertainty (that is due to chance). We introduce the epistemic neural network (ENN) as an interface for models that represent uncertainty as required to generate useful joint predictions. While prior approaches to uncertainty modeling such as Bayesian neural networks can be expressed as ENNs, this new interface facilitates comparison of joint predictions and the design of novel architectures and algorithms. In particular, we introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to represent uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. We demonstrate this efficacy across synthetic data, ImageNet, and some reinforcement learning tasks. As part of this effort we open-source experiment code.

## Biography

Vikranth is a research scientist at DeepMind in Mountain View. He is interested in developing data and computationally efficient agents. Before DeepMind, Vikranth completed his PhD at Stanford University with Prof. Benjamin Van Roy and Bachelor's at IIT Bombay with Prof. Vivek Borkar.

